{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3e099b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI news digest\n"
     ]
    }
   ],
   "source": [
    "print(\"AI news digest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f0a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "import requests \n",
    "import os \n",
    "from datetime import datetime, timedelta, timezone \n",
    "from dotenv import load_dotenv \n",
    "\n",
    "# load environment variables from .env file \n",
    "load_dotenv()\n",
    "\n",
    "class NewsFetcher:\n",
    "    def __init__(self, api_key=None):\n",
    "        self.api_key = api_key or os.getenv(\"NEWSAPI_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"NEWSAPI_KEY not found in environment variables or .env file\")\n",
    "        self.base_url = \"https://newsapi.org/v2/everything\"\n",
    "\n",
    "    def fetch_articles(self, query: str, num_articles: int = 5, days_back: int = 1, sources: str=\"\", language: str = \"en\") -> list:\n",
    "        \"\"\" \n",
    "        Fetch recent news articles based on user query \n",
    "        Returns list of artciles with: title, url, content, source, and publishedAt\n",
    "        \"\"\"\n",
    "        # calculate date range \n",
    "        to_date = datetime.now(timezone.utc)\n",
    "        from_date = to_date - timedelta(days=days_back)\n",
    "\n",
    "        params = {\n",
    "            \"q\": query, \n",
    "            \"pageSize\": num_articles, \n",
    "            \"from\": from_date.strftime(\"%Y-%m-%d\"),\n",
    "            \"to\": to_date.strftime(\"%Y-%m-%d\"),\n",
    "            \"language\": language,\n",
    "            \"sortBy\": \"relevancy\",\n",
    "            \"apiKey\": self.api_key # Make sure this is included \n",
    "        }\n",
    "\n",
    "        if sources:\n",
    "            params[\"sources\"] = sources \n",
    "\n",
    "        try:\n",
    "            response = requests.get(self.base_url, params=params)\n",
    "            # Check for 401 specifically \n",
    "            if response.status_code == 401:\n",
    "                print(\"401 Unauthorized: Check your API key\")\n",
    "                print(f\"Key key: {self.api_key[:3]}...{self.api_key[-3:]}\")\n",
    "                print(\"Verify your key at https://newsapi_org/account\")\n",
    "                return []\n",
    "            \n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            if data['status'] == \"ok\":\n",
    "                return [ \n",
    "                    {\n",
    "                        \"title\": article['title'],\n",
    "                        \"url\": article[\"url\"],\n",
    "                        \"content\": article[\"content\"] or article[\"description\"] or \"\",\n",
    "                        \"source\": article[\"source\"][\"name\"],\n",
    "                        \"published\": article[\"publishedAt\"]\n",
    "                    }\n",
    "                    for article in data[\"articles\"]\n",
    "                ]\n",
    "\n",
    "            else:\n",
    "                print(f\"API Error: {data.get('message', 'Unknown error')}\")\n",
    "                return []\n",
    "\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {str(e)}\")\n",
    "            return [] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc6c1cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 articles:\n",
      "\n",
      "Article 1: AI is raising the bar for sales ‚Äî and Microsoft's layoffs prove the 'relationship guy' is out, says a software investor\n",
      "Source: Business Insider\n",
      "URL: https://www.businessinsider.com/microsoft-layoffs-salespeople-relationship-guy-ai-solution-engineer-investor-2025-7\n",
      "Preview: Microsoft began culling less than 4% of its workforce, or about 9,000 employees, earlier this month,...\n",
      "\n",
      "Article 2: Shopify has quietly set boundaries for ‚Äòbuy-for-me‚Äô AI bots on merchant sites\n",
      "Source: Digiday\n",
      "URL: http://digiday.com/marketing/shopify-has-quietly-set-boundaries-for-buy-for-me-ai-bots-on-merchant-sites/\n",
      "Preview: Shopify is drawing a line in the sand on agentic AI a type of bot that autonomously completes tasks ...\n",
      "\n",
      "Article 3: Amazon challenges Microsoft with Kiro, its new AI-powered IDE\n",
      "Source: Neowin\n",
      "URL: https://www.neowin.net/news/amazon-challenges-microsoft-with-kiro-its-new-ai-powered-ide/\n",
      "Preview: The market for AI tools aimed at developers has become one of the most fiercely competitive areas in...\n",
      "\n",
      "Article 4: Middle East Startups Double Fundraising to Defy Broad Slowdown\n",
      "Source: Biztoc.com\n",
      "URL: https://biztoc.com/x/6417dc814fb26685\n",
      "Preview: { window.open(this.href, '_blank'); }, 200); return false;\"&gt;Why did Nvidia resume AI chip sales t...\n",
      "\n",
      "Article 5: US AI startups see funding surge while more VC funds struggle to raise, data shows\n",
      "Source: Biztoc.com\n",
      "URL: https://biztoc.com/x/6a20522f56cabfda\n",
      "Preview: { window.open(this.href, '_blank'); }, 200); return false;\"&gt;Why did Nvidia resume AI chip sales t...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    fetcher = NewsFetcher()\n",
    "    articles = fetcher.fetch_articles(\n",
    "        query=\"AI startups\",\n",
    "        num_articles=5, \n",
    "        days_back=1\n",
    "    )\n",
    "\n",
    "    print(f\"Found {len(articles)} articles:\")\n",
    "    for i, article in enumerate(articles, 1):\n",
    "        print(f\"\\nArticle {i}: {article['title']}\")\n",
    "        print(f\"Source: {article['source']}\")\n",
    "        print(f\"URL: {article['url']}\")\n",
    "        if article['content']:\n",
    "            print(f\"Preview: {article['content'][:100]}...\")\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"Configuration error: {str(e)}\")\n",
    "    print(\"Please create a .env file with NEWSAPI_KEY=your_api_key\")\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60cbe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from langchain_groq import ChatGroq \n",
    "from langchain.prompts import PromptTemplate \n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnablePassthrough \n",
    "from dotenv import load_dotenv \n",
    "\n",
    "# Load environment variables \n",
    "load_dotenv()\n",
    "\n",
    "class ArticleSummarizer:\n",
    "    def __init__(self, model_name=\"llama3-70b-8192\"):\n",
    "        \"\"\"  \n",
    "        Initialize Groq summarizer \n",
    "        Available model: \"llama3-70b-8192\", \"llama3-8b-8192\", \"mixtral-8x7b-32768\"\n",
    "        \"\"\"\n",
    "        self.groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "        if not self.groq_api_key:\n",
    "            raise ValueError(\"GROQ_aPI_KEY not found in .env file\")\n",
    "        \n",
    "        self.model = ChatGroq(\n",
    "            temperature=0.3,\n",
    "            model_name=model_name,\n",
    "            api_key=self.groq_api_key\n",
    "        )\n",
    "\n",
    "        # Configure text splitter for long articles \n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=12000, # Adjust based on model context window  \n",
    "            chunk_overlap=500,\n",
    "            length_function=len \n",
    "        )\n",
    "\n",
    "        # Define our summarization prompt \n",
    "        self.summary_prompt = PromptTemplate.from_template(\n",
    "            \"\"\"  You are an expert new analyst. Create a concise summary (1- paragraphs) of the following article:\n",
    "            \n",
    "            **Article Content:** \n",
    "            {content}\n",
    "            \n",
    "            **Summary Requirements:** \n",
    "            1. Identify the core insight or main claim \n",
    "            2. Extract all important named entities (people, organizations, locations)\n",
    "            3. Highlight key facts and figures \n",
    "            4. Maintain neutral, objective tone \n",
    "\n",
    "            **Output Format:** \n",
    "            - First paragraph: Core insight and key facts \n",
    "            - Second paragraph: Named entities and their significance \n",
    "\n",
    "            **Summary:**\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        # create summarization chain \n",
    "        self.summary_chain = (\n",
    "            {\"content\": RunnablePassthrough()}\n",
    "            | self.summary_prompt \n",
    "            | self.model\n",
    "            | StrOutputParser() \n",
    "        )\n",
    "    \n",
    "    def summarize(self, article_content: str) -> str:\n",
    "        \"\"\"  \n",
    "        Summarize article content handling long text \n",
    "        \"\"\"\n",
    "        # skip summarization if content is too short\n",
    "        if len(article_content) < 300:\n",
    "            return article_content \n",
    "        \n",
    "        # Handle long article with chunking \n",
    "        if len(article_content) > 8000:\n",
    "            chunks = self.text_splitter.split_text(article_content)\n",
    "            summaries = []\n",
    "\n",
    "            # Summarize each chunk \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk_summary = self.summary_chain.invoke(chunk)\n",
    "                summaries.append(f\"Part {i+1}: {chunk_summary}\")\n",
    "\n",
    "            # Combine chunk summaries \n",
    "            combined_content = \"\\n\\n\".join(summaries)\n",
    "            return self.summary_chain.invoke(combined_content)\n",
    "        \n",
    "        # Direct summarization for normal-length articles \n",
    "        return self.summary_chain.invoke(article_content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "425651c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Article 1: ‚ÄòSaturday Night Live‚Äô VFX Workers Ratify First Union Contract Via IATSE\n",
      "Souce: Deadline\n",
      "URL: http://deadline.com/2025/07/saturday-night-live-vfx-workers-ratify-first-contract-iatse-1236456965/\n",
      "\n",
      "Summary\n",
      "Visual effects workers at Saturday Night Liveare now officially operating under a union contract.\n",
      "The 15-member group voted unanimously in July to ratify its first union contract since organizing wi‚Ä¶ [+1185 chars]\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "Article 2: Swiss AI For Public Good: A ProSocial AI Blueprint For The World?\n",
      "Souce: Forbes\n",
      "URL: https://www.forbes.com/sites/corneliawalther/2025/07/14/swiss-ai-for-public-good-a-prosocial-ai-blueprint-for-the-world/\n",
      "\n",
      "Summary\n",
      "Cow in Swiss Montains\n",
      "getty\n",
      "Current Artificial intelligence development resembles a high-stakes race between tech giants. But Switzerland has chosen a different path. The Swiss AI Initiative's fort‚Ä¶ [+10100 chars]\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "Article 3: HealthMutual, MediConCen partner to develop AI\n",
      "Souce: Lifeinsuranceinternational.com\n",
      "URL: https://www.lifeinsuranceinternational.com/news/healthmutual-mediconcen-ai-claims-solution/\n",
      "\n",
      "Summary\n",
      "HealthMutual Group (HMG) has joined forces with insurtech company MediConCen to create an AI-driven claims solution for the Hong Kong market.\n",
      "The collaboration combines the technological competence ‚Ä¶ [+2182 chars]\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize components \n",
    "fetcher = NewsFetcher()\n",
    "summarizer = ArticleSummarizer(model_name=\"mixtral-8x7b-32768\")  # Faster model \n",
    "\n",
    "# Get articles on a topic \n",
    "articles = fetcher.fetch_articles(\n",
    "    query=\"AI in healthcare\",\n",
    "    num_articles=3, \n",
    "    days_back=2\n",
    ")\n",
    "\n",
    "# Summarize each article \n",
    "for i, article in enumerate(articles, 1):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Article {i}: {article['title']}\")\n",
    "    print(f\"Souce: {article['source']}\")\n",
    "    print(f\"URL: {article['url']}\")\n",
    "\n",
    "    # Summarize content \n",
    "    summary =summarizer.summarize(article['content'])\n",
    "    print(f\"\\nSummary\")\n",
    "    print(summary)\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b504c897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "import os \n",
    "import requests \n",
    "from langchain_groq import ChatGroq \n",
    "from langchain.prompts import PromptTemplate \n",
    "from langchain.schema import StrOutputParser \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "#from newspaper import Article, ArticleException\n",
    "import re \n",
    "import nltk \n",
    "import logging \n",
    "#from nltk.corpus import stopwords \n",
    "#from nltk.tokenize import sent_tokenize \n",
    "\n",
    "# Configure logging \n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s -%(levelname)s - %(message)s')\n",
    "\n",
    "# download required NLTK data \n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# load environment variables \n",
    "load_dotenv()\n",
    "\n",
    "class FullTextExtractor:\n",
    "    \"\"\"Robust text extraction without newspaper library\"\"\"\n",
    "    @staticmethod\n",
    "    def extract_text(url: str) -> str:\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "                'Accept-Language': 'en-US,en;q=0.5',\n",
    "                'Connection': 'keep-alive',\n",
    "                'Referer': 'https://www.google.com/'\n",
    "            }\n",
    "\n",
    "            # Try to bypass paywalls for specific sites\n",
    "            if \"businessinsider.com\" in url:\n",
    "                headers['Referer'] = 'https://www.facebook.com/'\n",
    "                headers['Cookie'] = 'bounceClientVisit=1; bounceClientFirstVisit=1'\n",
    "            \n",
    "            response = requests.get(url, headers=headers, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "            # Remove unwanted elements\n",
    "            for element in soup(['script', 'style', 'nav', 'footer', 'aside', 'form', 'header', \n",
    "                                 'iframe', 'button', 'svg', 'figure', 'noscript', 'img', 'link']):\n",
    "                element.decompose()\n",
    "\n",
    "            # Find main content using common selectors\n",
    "            selectors = [\n",
    "                'article', \n",
    "                'div.article-body',\n",
    "                'div.post-content',\n",
    "                'div.story-content',\n",
    "                'div.entry-content',\n",
    "                'div.content-wrapper',\n",
    "                'div.main-content',\n",
    "                'section.main',\n",
    "                'div.article-content',\n",
    "                'div#article-body',\n",
    "                'div.article-text',\n",
    "                'div.post-body'\n",
    "            ]\n",
    "            \n",
    "            article_body = None\n",
    "            for selector in selectors:\n",
    "                article_body = soup.select_one(selector)\n",
    "                if article_body:\n",
    "                    break\n",
    "\n",
    "            # Fallback to body if no specific content found\n",
    "            if not article_body:\n",
    "                article_body = soup.body\n",
    "\n",
    "            # Extract text with paragraph structure\n",
    "            text = \"\"\n",
    "            for element in article_body.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'blockquote']):\n",
    "                if element.name == 'p':\n",
    "                    text += element.get_text().strip() + \"\\n\\n\"\n",
    "                elif element.name == 'blockquote':\n",
    "                    text += f\"> {element.get_text().strip()}\\n\\n\"\n",
    "                else:  # Headings\n",
    "                    text += f\"\\n\\n{element.get_text().strip().upper()}\\n\\n\"\n",
    "\n",
    "            # Clean and compress text\n",
    "            text = re.sub(r'\\n{3,}', '\\n\\n', text)  # Remove excessive newlines\n",
    "            text = re.sub(r'\\[\\+[0-9,]+\\s*chars?\\]', '', text)  # Remove truncation markers\n",
    "            return text.strip()\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Extraction failed for {url}: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "class ArticleSummarizer:\n",
    "    def __init__(self, model_name=\"llama3-70b-8192\"):\n",
    "        self.groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "        if not self.groq_api_key:\n",
    "            raise ValueError(\"GROQ_API_KEY not found in .env file\")\n",
    "        \n",
    "        self.model = ChatGroq(\n",
    "            temperature=0.3,\n",
    "            model_name=model_name,\n",
    "            api_key=self.groq_api_key\n",
    "        )\n",
    "\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=8000,\n",
    "            chunk_overlap=300,\n",
    "            length_function=len \n",
    "        )\n",
    "\n",
    "        self.summary_prompt = PromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            Create a professional 2-paragraph news summary from the following article content.\n",
    "            Follow these guidelines:\n",
    "            1. Omit any introductory phrases like \"Here is a summary\"\n",
    "            2. First paragraph: Core innovation/event and key facts \n",
    "            3. Second paragraph: Key entities and business implications \n",
    "            4. Include specific numbers and metrics when available \n",
    "            5. Maintain jouranlistic tone\n",
    "\n",
    "            Example structure:\n",
    "            [Company] has [achievement] using [technology]. The development [specific impact]... \n",
    "            Key players include [names] from [organizations]. This could [business implication]...\n",
    "\n",
    "            Article Content:\n",
    "            {content}\n",
    "\n",
    "            Professional Summary: \n",
    "\n",
    "            \"\"\" \n",
    "        )\n",
    "\n",
    "        self.summary_chain = (\n",
    "            {\"content\": RunnablePassthrough()}\n",
    "            | self.summary_prompt\n",
    "            | self.model\n",
    "            |StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def summarize(self, article: dict) -> str:\n",
    "        \"\"\"Robust summarization with multiple fallbacks\"\"\"\n",
    "        # Get full article content \n",
    "        full_text = FullTextExtractor.extract_text(article['url'])\n",
    "\n",
    "        # Use snippet if full text extraction failed \n",
    "        if not full_text.strip() or len(full_text) < 300:\n",
    "            logging.warning(f\"Using snippet for {article['url']}\")\n",
    "            full_text = self.clean_snippet(article['content'])\n",
    "            if len(full_text) < 100:\n",
    "                return \"Summary unavailable: Could not retrieve content\"\n",
    "            \n",
    "        # Clean and prepare text \n",
    "        clean_text = self.preprocess_text(full_text)\n",
    "        logging.info(f\"Processing text: {len(clean_text)} characters\")\n",
    "        \n",
    "        # Handle long articles with chunking \n",
    "        if len(clean_text) > 8000:\n",
    "            chunks = self.text_splitter.split_text(clean_text)\n",
    "            chunk_summaries = []\n",
    "\n",
    "            for i, chunk in enumerate(chunks, 1):\n",
    "                logging.info(f\"Summarizing chunk {i}/{len(chunks)}\")\n",
    "                chunk_summaries.append(self.summarize_chunk(chunk))\n",
    "                \n",
    "            combined_content = \"\\n\\n\".join(chunk_summaries)\n",
    "            return self.summarize_chunk(combined_content)\n",
    "        \n",
    "        return self.summarize_chunk(clean_text)\n",
    "     \n",
    "    def summarize_chunk(self, text: str) -> str:\n",
    "        \"\"\"Handle single chunk summarization with error recovery\"\"\"\n",
    "        try:\n",
    "            return self.summary_chain.invoke(text)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Summarization error: {str(e)}\")\n",
    "            return \"Summary generation failed\" \n",
    "        \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Clean text before processing\"\"\"\n",
    "        # Remove common boilerplate\n",
    "        patterns = [\n",
    "            r\"Sign up for.*newsletters\",\n",
    "            r\"Subscribe to.*channel\",\n",
    "            r\"Follow us on.*\",\n",
    "            r\"Download our.*app\",\n",
    "            r\"Read more:.*\",\n",
    "            r\"Continue reading.*\",\n",
    "            r\"Advertisement\",\n",
    "            r\"Recommended for you\",\n",
    "            r\"Related:.*\",\n",
    "            r\"Please enter your email\",\n",
    "            r\"Already have an account\\? Log in\",\n",
    "            r\"Create a free account\",\n",
    "            r\"¬© Copyright.*\"\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        for pattern in patterns:\n",
    "            text = re.sub(pattern, \"\", text, flags=re.IGNORECASE)\n",
    "        \n",
    "        return text.strip()\n",
    "        \n",
    "    def clean_snippet(self, snippet: str) -> str:\n",
    "        \"\"\"Clean NewsAPI snippets\"\"\"\n",
    "        # Remove truncation markers\n",
    "        snippet = re.sub(r'\\[\\+[0-9,]+\\s*chars?\\]', '', snippet)\n",
    "        # Remove HTML tags\n",
    "        snippet = re.sub(r'<[^>]+>', '', snippet)\n",
    "        return snippet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "819f047b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Article 1: Tesla stock needs 'Elon Musk magic,' expert says\n",
      "Source: Yahoo Entertainment\n",
      "URL: https://finance.yahoo.com/video/tesla-stock-needs-elon-musk-100024661.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 11:52:52,835 -INFO - Processing text: 3501 characters\n",
      "2025-07-16 11:52:54,285 -INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Summary:\n",
      "Tesla's stock valuation is heavily reliant on \"Elon Musk magic,\" according to Tufts University Fletcher School of Law and Diplomacy adjunct associate professor Gautam Mukunda. Musk's recent announcement to launch a new third party has sparked concerns among investors and analysts, who fear that the CEO's distraction from the company's core business could negatively impact the stock. Mukunda argues that Tesla's current valuation is attributed to Musk's ability to drive innovation and growth, and that his involvement is crucial to the company's success.\n",
      "\n",
      "Key players, including Musk and investors, are closely watching the situation unfold. If Musk is not fully focused on Tesla, the company's valuation could take a hit. Mukunda suggests that investors who are ignoring the drama and focusing solely on Tesla's electric vehicle sales may need to reevaluate their strategy, as the company's dominance in the electric car market is being challenged by rivals producing high-quality cars. The expert's comments come as Tesla's Q2 deliveries exceeded expectations, causing the stock to pop, but the company's long-term success remains uncertain.\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "Article 2: 3 Stocks Estimated To Be Trading At A Discount Of Up To 25.8%\n",
      "Source: Yahoo Entertainment\n",
      "URL: https://finance.yahoo.com/news/3-stocks-estimated-trading-discount-113751758.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 11:52:55,311 -INFO - Processing text: 6384 characters\n",
      "2025-07-16 11:52:56,489 -INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Summary:\n",
      "Here is a 2-paragraph professional news summary:\n",
      "\n",
      "Roku, Robert Half, and Ligand Pharmaceuticals are among the top 10 undervalued stocks in the US market, with estimated discounts of up to 49.4% based on cash flows. The US market has remained flat over the past week, but has shown an impressive 11% increase over the past year with earnings forecasted to grow by 15% annually. Identifying undervalued stocks can offer potential value opportunities for investors looking to capitalize on future growth prospects.\n",
      "\n",
      "Key players include Mr. Cooper Group, Apollo Global Management, and Lazard, which are trading at discounts of up to 25.8% to their estimated fair values. These companies have strong growth prospects, with forecasted annual earnings growth rates ranging from 18.93% to 24.8%. Despite some challenges, these companies have the potential to outperform the broader US market, making them attractive opportunities for investors seeking value and growth.\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "Article 3: Rigetti Computing (NasdaqCM:RGTI) Completes $350M Equity Offering\n",
      "Source: Yahoo Entertainment\n",
      "URL: https://finance.yahoo.com/news/rigetti-computing-nasdaqcm-rgti-completes-173309457.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 11:52:57,608 -INFO - Processing text: 2923 characters\n",
      "2025-07-16 11:52:58,521 -INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Summary:\n",
      "Rigetti Computing (NASDAQCM:RGTI) has completed a $350 million equity offering, coinciding with a 41% share price increase over the last quarter. This significant valuation change is attributed to the company's strategic realignment under growth benchmarks, collaborative efforts in quantum technology, and successful capital raising initiatives.\n",
      "\n",
      "Key players include Rigetti Computing, which has achieved a total return of over 900% in the past year, outperforming the broader US market and the US Semiconductor industry. The company's transition to growth-oriented benchmarks may positively impact its revenue and earnings forecasts, with analysts expecting potential revenue growth despite remaining unprofitable in the near future. With the current share price below the consensus analyst target, a potential uplift of around 21.8% is forecasted.\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize components \n",
    "fetcher = NewsFetcher()\n",
    "\n",
    "summarizer = ArticleSummarizer(model_name=\"llama3-70b-8192\")\n",
    "\n",
    "# Get articles \n",
    "articles = fetcher.fetch_articles(\n",
    "    query=\"Stock Market\",\n",
    "    num_articles=3,\n",
    "    days_back=2\n",
    ")\n",
    "\n",
    "# Process each article \n",
    "for i, article in enumerate(articles, 1):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Article {i}: {article['title']}\")\n",
    "    print(f\"Source: {article['source']}\")\n",
    "    print(f\"URL: {article['url']}\")\n",
    "\n",
    "    try:\n",
    "        # get enhanced summary \n",
    "        summary = summarizer.summarize(article)\n",
    "        print(f\"\\nüìù Summary:\")\n",
    "        print(summary)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Summarization failed: {str(e)}\")\n",
    "\n",
    "    print(f\"{'='*50}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cf043eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "class FullTextExtractor:\n",
    "    \"\"\"Improved full-text extraction with better content detection\"\"\"\n",
    "    @staticmethod\n",
    "    def extract_text(url: str) -> str:\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "                'Accept-Language': 'en-US,en;q=0.5',\n",
    "                'Connection': 'keep-alive'\n",
    "            }\n",
    "            response = requests.get(url, headers=headers, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Remove unwanted elements\n",
    "            for element in soup(['script', 'style', 'nav', 'footer', 'aside', 'form', 'header', \n",
    "                                 'iframe', 'button', 'svg', 'figure', 'noscript']):\n",
    "                element.decompose()\n",
    "                \n",
    "            # Find main content using common selectors\n",
    "            selectors = [\n",
    "                'article', \n",
    "                'div.article-body',\n",
    "                'div.post-content',\n",
    "                'div.story-content',\n",
    "                'div.entry-content',\n",
    "                'div.content-wrapper',\n",
    "                'div.main-content',\n",
    "                'section.main'\n",
    "            ]\n",
    "            \n",
    "            article_body = None\n",
    "            for selector in selectors:\n",
    "                article_body = soup.select_one(selector)\n",
    "                if article_body:\n",
    "                    break\n",
    "                    \n",
    "            # Fallback to body if no specific content found\n",
    "            if not article_body:\n",
    "                article_body = soup.body\n",
    "                \n",
    "            # Extract text with paragraph structure\n",
    "            text = \"\"\n",
    "            for element in article_body.find_all(['p', 'h1', 'h2', 'h3']):\n",
    "                if element.name == 'p':\n",
    "                    text += element.get_text().strip() + \"\\n\\n\"\n",
    "                else:  # Headings\n",
    "                    text += f\"\\n\\n{element.get_text().strip().upper()}\\n\\n\"\n",
    "            \n",
    "            # Clean and compress text\n",
    "            text = re.sub(r'\\n{3,}', '\\n\\n', text)  # Remove excessive newlines\n",
    "            return text.strip()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from {url}: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "class ArticleSummarizer:\n",
    "    def __init__(self, model_name=\"llama3-70b-8192\"):\n",
    "        self.groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "        if not self.groq_api_key:\n",
    "            raise ValueError(\"GROQ_API_KEY not found in .env file\")\n",
    "            \n",
    "        self.model = ChatGroq(\n",
    "            temperature=0.3,\n",
    "            model_name=model_name,\n",
    "            api_key=self.groq_api_key\n",
    "        )\n",
    "        \n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=8000,\n",
    "            chunk_overlap=300,\n",
    "            length_function=len\n",
    "        )\n",
    "        \n",
    "        self.summary_prompt = PromptTemplate.from_template(\n",
    "            \"\"\"You are an expert news analyst. Create a concise 2-paragraph summary of the article below.\n",
    "            Focus on these key elements:\n",
    "            1. The core innovation, event, or main claim\n",
    "            2. Key people/organizations involved\n",
    "            3. Significant data points or metrics\n",
    "            4. Business or industry implications\n",
    "            \n",
    "            Structure your summary as:\n",
    "            - First paragraph: Core insight and key facts\n",
    "            - Second paragraph: Named entities and significance\n",
    "            \n",
    "            If the article content is not available, return only: \"Summary unavailable\"\n",
    "            \n",
    "            Article Content:\n",
    "            {content}\n",
    "            \n",
    "            Summary:\"\"\"\n",
    "        )\n",
    "        \n",
    "        self.summary_chain = (\n",
    "            {\"content\": RunnablePassthrough()} \n",
    "            | self.summary_prompt\n",
    "            | self.model\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "    \n",
    "    def summarize(self, article: dict) -> str:\n",
    "        \"\"\"Summarize article by extracting full text first\"\"\"\n",
    "        # Get full article content\n",
    "        full_text = FullTextExtractor.extract_text(article['url'])\n",
    "        \n",
    "        # Use snippet if full text extraction failed\n",
    "        if not full_text.strip():\n",
    "            print(f\"‚ö†Ô∏è Extraction failed for: {article['url']}\")\n",
    "            print(\"Attempting to use NewsAPI snippet...\")\n",
    "            full_text = article['content']\n",
    "        \n",
    "        # If we still don't have text, return error\n",
    "        if not full_text.strip():\n",
    "            return \"Summary unavailable: Could not retrieve article content\"\n",
    "            \n",
    "        print(f\"\\nExtracted content length: {len(full_text)} characters\")\n",
    "        \n",
    "        # Pre-process text to remove boilerplate\n",
    "        full_text = self.remove_boilerplate(full_text)\n",
    "        \n",
    "        # Summarize with Groq LLM\n",
    "        if len(full_text) > 7000:\n",
    "            print(\"Article too long, chunking...\")\n",
    "            chunks = self.text_splitter.split_text(full_text)\n",
    "            chunk_summaries = []\n",
    "            \n",
    "            for i, chunk in enumerate(chunks, 1):\n",
    "                print(f\"Summarizing chunk {i}/{len(chunks)}...\")\n",
    "                chunk_summaries.append(self.summary_chain.invoke(chunk))\n",
    "                \n",
    "            combined_content = \"\\n\\n\".join(chunk_summaries)\n",
    "            print(\"Creating final summary from chunks...\")\n",
    "            return self.summary_chain.invoke(combined_content)\n",
    "        \n",
    "        return self.summary_chain.invoke(full_text)\n",
    "    \n",
    "    def remove_boilerplate(self, text: str) -> str:\n",
    "        \"\"\"Remove common boilerplate text\"\"\"\n",
    "        patterns = [\n",
    "            r\"Sign up for.*newsletters\",\n",
    "            r\"Subscribe to.*channel\",\n",
    "            r\"Follow us on.*\",\n",
    "            r\"Download our.*app\",\n",
    "            r\"Read more:.*\",\n",
    "            r\"Continue reading.*\",\n",
    "            r\"\\[.*chars\\]\",\n",
    "            r\"Advertisement\",\n",
    "            r\"Recommended for you\",\n",
    "            r\"Related:.*\"\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            text = re.sub(pattern, \"\", text, flags=re.IGNORECASE)\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42e1f327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Article 1: Perplexity's engineers use 2 AI coding tools, and they've cut development time from days to hours\n",
      "Source: Business Insider\n",
      "URL: https://www.businessinsider.com/perplexity-engineers-ai-tools-cut-development-time-days-hours-2025-7\n",
      "\n",
      "Extracted content length: 62 characters\n",
      "\n",
      "üìù Summary:\n",
      "Summary unavailable\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "Article 2: How Google Killed OpenAI‚Äôs $3 Billion Deal Without an Acquisition\n",
      "Source: Gizmodo.com\n",
      "URL: https://gizmodo.com/how-google-killed-openais-3-billion-deal-without-an-acquisition-2000628693\n",
      "\n",
      "Extracted content length: 6112 characters\n",
      "\n",
      "üìù Summary:\n",
      "Here is a 2-paragraph summary of the article:\n",
      "\n",
      "Google has dealt a significant blow to OpenAI by poaching key talent from Windsurf, a startup that had a reported $3 billion acquisition deal with OpenAI. Instead, Google is paying $2.4 billion to hire top Windsurf employees, including the CEO, and take a non-exclusive license to its technology. This move solidifies a rising trend in Silicon Valley's AI arms race, known as the \"non-acquisition acquisition\" or \"acqui-hire,\" where big tech companies poach top talent from startups without acquiring the company itself.\n",
      "\n",
      "The key players involved in this trend include Google, OpenAI, Meta, Microsoft, and Amazon, all of which have made significant acqui-hire deals in the past year. This strategy allows big tech companies to gain access to valuable AI technology and top research talent while sidestepping antitrust scrutiny. The implications of this trend are significant, as it could shape the future of the AI industry and American big tech. The Federal Trade Commission (FTC) will need to define its stance on this practice, which could have far-reaching consequences for the industry.\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "Article 3: Hypercapitalism and the AI Talent Wars\n",
      "Source: Johnluttig.com\n",
      "URL: https://blog.johnluttig.com/p/hypercapitalism-and-the-ai-talent\n",
      "\n",
      "Extracted content length: 16693 characters\n",
      "Article too long, chunking...\n",
      "Summarizing chunk 1/3...\n",
      "Summarizing chunk 2/3...\n",
      "Summarizing chunk 3/3...\n",
      "Creating final summary from chunks...\n",
      "\n",
      "üìù Summary:\n",
      "Here is a 2-paragraph summary of the article:\n",
      "\n",
      "The core innovation is the emerging trend of concentration of outcomes in the tech industry, driven by the intense competition for top AI talent. This has led to skyrocketing compensation packages, with companies like Meta and Google offering multi-hundred million dollar deals to attract the best talent. As a result, the industry is shifting towards a hypercapitalist model, where the top 1% of companies drive the majority of venture capital returns, and a small group of individuals have a disproportionate impact.\n",
      "\n",
      "Key players involved in this talent war include Meta, Google, Founders Fund, OpenAI, and Anthropic, with notable individuals like Jony Ive, Jeff Dean, Andy Jassy, Axel Ericsson, Philip Clark, and Joey Krug driving billions of dollars in value. The implications of this trend are significant, with the potential to fundamentally change the way companies operate, leading to a rewriting of employment contracts and investment norms, and the emergence of new labor dynamics, such as agents and unions, to protect against mercenary market forces.\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize components\n",
    "fetcher = NewsFetcher()\n",
    "summarizer = ArticleSummarizer(model_name=\"llama3-70b-8192\")\n",
    "    \n",
    "# Get articles\n",
    "articles = fetcher.fetch_articles(\n",
    "    query=\"AI Startups\",\n",
    "    num_articles=3,\n",
    "    days_back=2\n",
    ")\n",
    "    \n",
    "# Process each article\n",
    "for i, article in enumerate(articles, 1):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Article {i}: {article['title']}\")\n",
    "    print(f\"Source: {article['source']}\")\n",
    "    print(f\"URL: {article['url']}\")\n",
    "        \n",
    "    try:\n",
    "        # Get enhanced summary\n",
    "        summary = summarizer.summarize(article)\n",
    "        print(f\"\\nüìù Summary:\")\n",
    "        print(summary)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Summarization failed: {str(e)}\")\n",
    "        \n",
    "    print(f\"{'='*50}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3276b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 \n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self, model_name=\"llama3-70b-8192\"):\n",
    "        self.groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "        if not self.groq_api_key:\n",
    "            raise ValueError(\"GROQ_API_KEY not found in .env file\")\n",
    "        \n",
    "        self.model = ChatGroq(\n",
    "            temperature=0.1,  # Lower temperature for classification\n",
    "            model_name=model_name,\n",
    "            api_key=self.groq_api_key\n",
    "        )\n",
    "        self.sentiment_prompt = PromptTemplate.from_template(\n",
    "            \"\"\"Classify the sentiment of the following news summary as POSITIVE, NEGATIVE, or NEUTRAL.\n",
    "            Consider these guidelines:\n",
    "            1. POSITIVE: Describes growth, success, breakthroughs, or favorable outcomes\n",
    "            2. NEGATIVE: Describes failures, controversies, losses, or unfavorable outcomes\n",
    "            3. NEUTRAL: Balanced reporting, announcements without clear positive/negative slant\n",
    "            \n",
    "            Respond ONLY with one word: POSITIVE, NEGATIVE, or NEUTRAL\n",
    "            \n",
    "            News Summary:\n",
    "            {summary}\n",
    "            Sentiment:\"\"\"\n",
    "        )\n",
    "        self.sentiment_chain = (\n",
    "            {\"summary\": RunnablePassthrough()} \n",
    "            | self.sentiment_prompt\n",
    "            | self.model\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def analyze(self, summary: str) -> str:\n",
    "        \"\"\"Analyze sentiment of a news summary\"\"\"\n",
    "        if \"unavailable\" in summary.lower() or len(summary) < 20:\n",
    "            return \"NEUTRAL\"\n",
    "        \n",
    "        try:\n",
    "            sentiment = self.sentiment_chain.invoke(summary)\n",
    "            # Clean and standardize the output\n",
    "            sentiment = sentiment.strip().upper()\n",
    "            if \"POSITIVE\" in sentiment:\n",
    "                return \"POSITIVE\"\n",
    "            elif \"NEGATIVE\" in sentiment:\n",
    "                return \"NEGATIVE\"\n",
    "            return \"NEUTRAL\"\n",
    "        except Exception as e:\n",
    "            print(f\"Sentiment analysis failed: {str(e)}\")\n",
    "            return \"NEUTRAL\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cd0212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 \n",
    "class DailyDigestGenerator:\n",
    "    def __init__(self, topic: str):\n",
    "        self.topic = topic\n",
    "\n",
    "    def generate(self, articles: list[dict]) -> str:\n",
    "        date_str = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # Count sentiment distribution\n",
    "        sentiment_counts = {\"POSITIVE\": 0, \"NEGATIVE\": 0, \"NEUTRAL\": 0}\n",
    "        for article in articles:\n",
    "            sentiment = article.get('sentiment', 'NEUTRAL')\n",
    "            sentiment_counts.setdefault(sentiment, 0)\n",
    "            sentiment_counts[sentiment] += 1\n",
    "\n",
    "        # Create sentiment summary\n",
    "        sentiment_summary = (\n",
    "            f\"üî• {sentiment_counts['POSITIVE']} Positive | \"\n",
    "            f\"‚ö†Ô∏è {sentiment_counts['NEUTRAL']} Neutral | \"\n",
    "            f\"‚ö° {sentiment_counts['NEGATIVE']} Negative\"\n",
    "        )\n",
    "\n",
    "        # Create key takeaways\n",
    "        takeaways = []\n",
    "        for article in articles:\n",
    "            emoji = (\n",
    "                \"üî•\" if article['sentiment'] == \"POSITIVE\" else\n",
    "                \"‚ö°\" if article['sentiment'] == \"NEGATIVE\" else\n",
    "                \"‚ö†Ô∏è\"\n",
    "            )\n",
    "            takeaways.append(\n",
    "                f\"{emoji} {article['title']} ({article['sentiment']})\\n\"\n",
    "                f\"   - {article['summary']}\\n\"\n",
    "                f\"   - Source: {article['source']}\"\n",
    "            )\n",
    "        # Generate digest\n",
    "        lines = [\n",
    "            f\"DAILY NEWS DIGEST: {self.topic.upper()}\",\n",
    "            f\"Date: {date_str}\",\n",
    "            f\"Articles: {len(articles)}\", \n",
    "            sentiment_summary,\n",
    "            \"\",\n",
    "            \"KEY TAKEAWAYS:\"\n",
    "        ]\n",
    "        for tk in takeaways:\n",
    "            lines.append(f\"‚Ä¢ {tk}\")\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"SOURCES:\")\n",
    "        for i, article in enumerate(articles, 1):\n",
    "            lines.append(f\"[{i}] {article['url']}\")\n",
    "\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d718b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "class DailyDigestGenerator:\n",
    "    def __init__(self, topic: str):\n",
    "        self.topic = topic\n",
    "        self.date = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "    def generate(self, articles: list) -> str:\n",
    "        \"\"\"Generate daily digest from processed articles\"\"\"\n",
    "        # Count sentiment distribution\n",
    "        sentiment_counts = {\"POSITIVE\": 0, \"NEGATIVE\": 0, \"NEUTRAL\": 0}\n",
    "        for article in articles:\n",
    "            sentiment_counts[article['sentiment']] += 1\n",
    "        \n",
    "        # Create sentiment summary\n",
    "        sentiment_summary = (\n",
    "            f\"üî• {sentiment_counts['POSITIVE']} Positive | \"\n",
    "            f\"‚ö†Ô∏è {sentiment_counts['NEUTRAL']} Neutral | \"\n",
    "            f\"‚ö° {sentiment_counts['NEGATIVE']} Negative\"\n",
    "        )\n",
    "        \n",
    "        # Create key takeaways\n",
    "        takeaways = []\n",
    "        for i, article in enumerate(articles, 1):\n",
    "            emoji = \"üî•\" if article['sentiment'] == \"POSITIVE\" else \"‚ö°\" if article['sentiment'] == \"NEGATIVE\" else \"‚ö†Ô∏è\"\n",
    "            # Truncate summary if too long\n",
    "            summary = article['summary']\n",
    "            if len(summary) > 300:\n",
    "                summary = summary[:297] + \"...\"\n",
    "                \n",
    "            takeaways.append(\n",
    "                f\"{emoji} {article['title']} ({article['sentiment']})\\n\"\n",
    "                f\"   - {summary}\\n\"\n",
    "                f\"   - Source: {article['source']}\"\n",
    "            )\n",
    "        \n",
    "        # Generate digest\n",
    "        digest = f\"\"\"\n",
    "DAILY NEWS DIGEST: {self.topic.upper()}\n",
    "Date: {self.date}\n",
    "Articles: {len(articles)}\n",
    "Sentiment: {sentiment_summary}\n",
    "\n",
    "KEY TAKEAWAYS:\n",
    "{\"\".join([f'\\n‚Ä¢ {tk}' for tk in takeaways])}\n",
    "\n",
    "SOURCES:\n",
    "{\"\".join([f'\\n[{i}] {article[\"url\"]}' for i, article in enumerate(articles, 1)])}\n",
    "\"\"\"\n",
    "        \n",
    "        return digest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fc0dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 12:19:45,047 -WARNING - Using snippet for https://www.businessinsider.com/microsoft-layoffs-salespeople-relationship-guy-ai-solution-engineer-investor-2025-7\n",
      "2025-07-16 12:19:45,049 -INFO - Processing text: 192 characters\n",
      "2025-07-16 12:19:45,927 -INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-16 12:19:46,303 -INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-16 12:19:46,708 -INFO - Processing text: 4766 characters\n",
      "2025-07-16 12:19:47,768 -INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-16 12:19:48,114 -INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-16 12:19:49,900 -INFO - Processing text: 2088 characters\n",
      "2025-07-16 12:19:50,960 -INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-16 12:19:51,349 -INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-16 12:19:51,550 -INFO - Processing text: 4317 characters\n",
      "2025-07-16 12:19:52,651 -INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-16 12:19:52,994 -INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-16 12:19:53,375 -WARNING - Using snippet for https://biztoc.com/x/6417dc814fb26685\n",
      "2025-07-16 12:19:53,377 -INFO - Processing text: 200 characters\n",
      "2025-07-16 12:19:54,417 -INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-16 12:19:54,814 -INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAILY NEWS DIGEST: AI STARTUPS\n",
      "Date: 2025-07-16\n",
      "Articles: 5\n",
      "üî• 3 Positive | ‚ö†Ô∏è 2 Neutral | ‚ö° 0 Negative\n",
      "\n",
      "KEY TAKEAWAYS:\n",
      "‚Ä¢ ‚ö†Ô∏è AI is raising the bar for sales ‚Äî and Microsoft's layoffs prove the 'relationship guy' is out, says a software investor (NEUTRAL)\n",
      "   - Here is a professional 2-paragraph news summary:\n",
      "\n",
      "Microsoft has initiated a workforce reduction, laying off approximately 9,000 employees, which accounts for less than 4% of its total workforce. The majority of those affected are generalist sales representatives. This move is part of the company's efforts to restructure and adapt to changing market conditions.\n",
      "\n",
      "Key players involved in this decision include Microsoft's top executives, who are driving the company's strategic shift. This significant downsizing is expected to have a ripple effect on the tech industry, potentially influencing the job market and talent acquisition strategies of other major players in the sector.\n",
      "   - Source: Business Insider\n",
      "‚Ä¢ ‚ö†Ô∏è Shopify has quietly set boundaries for ‚Äòbuy-for-me‚Äô AI bots on merchant sites (NEUTRAL)\n",
      "   - Shopify has quietly set boundaries for 'buy-for-me' AI bots on merchant sites, updating its robots.txt file to prohibit automated scraping, 'buy-for-me' agents, or any end-to-end flow that completes payment without a final review step. This change, spotted across Shopify storefronts including Alo Yoga, Allbirds, and Brooklinen, aims to block agentic AI systems that autonomously complete tasks without human inputs.\n",
      "\n",
      "Key players include Shopify, Amazon, and Walmart, which are all exploring agentic AI capabilities. Shopify's move is seen as a way to protect its ecosystem by discouraging unauthorized AI scraping and checkout automation, while also promoting its official Checkout Kit for legitimate integrators. This development could have significant implications for the e-commerce industry, as retailers and platforms navigate the rules of engagement for AI and automation.\n",
      "   - Source: Digiday\n",
      "‚Ä¢ üî• Amazon challenges Microsoft with Kiro, its new AI-powered IDE (POSITIVE)\n",
      "   - Amazon has launched Kiro, an AI-powered agentic IDE built on top of Code OSS, a VSCode fork, to help developers turn prototypes into production-ready systems. Kiro introduces two key features, Specs and Hooks, which enable AI agents to produce better implementations and automate tasks in the background. The IDE can automatically generate design documents, data flow diagrams, and other project assets, and provides a task interface for initiating and reviewing AI-driven tasks.\n",
      "\n",
      "Key players in this development include Amazon, Microsoft, and OpenAI, among others, as the market for AI tools aimed at developers becomes increasingly competitive. With Kiro, Amazon is poised to gain a significant foothold in this market, offering a free preview period with generous usage limits, followed by Free, Pro, and Pro+ plans with varying levels of usage allowances. This move could have significant implications for the tech industry, as major players continue to invest in AI-powered development tools.\n",
      "   - Source: Neowin\n",
      "‚Ä¢ üî• This aerospike rocket engine designed by generative AI just completed its first hot fire test (POSITIVE)\n",
      "   - Here is a 2-paragraph professional news summary:\n",
      "\n",
      "Hyperganic and AMCM have successfully completed the first hot fire test of a 3D printed aerospike engine, designed using AI's generative skills. The 800 mm engine, printed in a copper alloy, achieved 5,000 Newtons of thrust during the test, powered by a fuel mix of cryogenic liquid oxygen and kerosene. This marks a significant milestone in the development of aerospike engines, which are notoriously difficult to design and manufacture.\n",
      "\n",
      "Key players in this achievement include Noyron AI, which used its computational AI to develop the engine in just a few weeks, and LEAP 71, whose CEO and Co-Founder, Josefine Lissner, credits Noyron with enabling the complex design. The successful test has significant implications for the aerospace industry, where aerospike engines could potentially replace multiple engines designed for different conditions. With further testing slated for 2025, this innovation could lead to more efficient and cost-effective space access.\n",
      "   - Source: PC Gamer\n",
      "‚Ä¢ üî• Middle East Startups Double Fundraising to Defy Broad Slowdown (POSITIVE)\n",
      "   - Here is a professional 2-paragraph news summary:\n",
      "\n",
      "Nvidia has resumed sales of its artificial intelligence (AI) chips to Chinese companies, following a temporary halt in October due to US export restrictions. The decision comes after the US Department of Commerce granted the company a license to export its AI chips, which are used in data centers and other applications. The sales are expected to generate significant revenue for Nvidia, with the Chinese market accounting for a substantial portion of its business.\n",
      "\n",
      "Key players involved in the decision include Nvidia CEO Jensen Huang and US Commerce Secretary Wilbur Ross. The resumption of AI chip sales is expected to have significant implications for the tech industry, particularly in the areas of AI development and data center infrastructure. With the Chinese market now open to Nvidia's AI chips, the company is poised to regain its market share and revenue growth in the region.\n",
      "   - Source: Biztoc.com\n",
      "\n",
      "SOURCES:\n",
      "[1] https://www.businessinsider.com/microsoft-layoffs-salespeople-relationship-guy-ai-solution-engineer-investor-2025-7\n",
      "[2] http://digiday.com/marketing/shopify-has-quietly-set-boundaries-for-buy-for-me-ai-bots-on-merchant-sites/\n",
      "[3] https://www.neowin.net/news/amazon-challenges-microsoft-with-kiro-its-new-ai-powered-ide/\n",
      "[4] https://www.pcgamer.com/hardware/this-aerospike-rocket-engine-designed-by-generative-ai-just-completed-its-first-hot-fire-test/\n",
      "[5] https://biztoc.com/x/6417dc814fb26685\n",
      "\n",
      "Digest saved to news_digest_20250716.txt\n"
     ]
    }
   ],
   "source": [
    "# 5\n",
    "def main():\n",
    "    load_dotenv()\n",
    "    topic = \"AI Startups\"\n",
    "\n",
    "    fetcher = NewsFetcher()\n",
    "    summarizer = ArticleSummarizer()\n",
    "    sentiment_analyzer = SentimentAnalyzer(model_name=\"llama3-8b-8192\")\n",
    "    digest_generator = DailyDigestGenerator(topic)\n",
    "\n",
    "    # Get and process articles\n",
    "    raw_articles = fetcher.fetch_articles(query=topic, num_articles=5, days_back=1)\n",
    "    processed_articles = []\n",
    "\n",
    "    for article in raw_articles:\n",
    "        summary = summarizer.summarize(article)\n",
    "        sentiment = sentiment_analyzer.analyze(summary)\n",
    "        processed_articles.append({\n",
    "            \"title\": article['title'],\n",
    "            \"source\": article['source'],\n",
    "            \"url\": article['url'],\n",
    "            \"summary\": summary,\n",
    "            \"sentiment\": sentiment\n",
    "        })\n",
    "\n",
    "    # Generate and save digest\n",
    "    digest = digest_generator.generate(processed_articles)\n",
    "    print(digest)\n",
    "    filename = f\"news_digest_{datetime.now(timezone.utc).strftime('%Y%m%d')}.txt\"\n",
    "    # Open file with utf-8 encoding to support emojis\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(digest)\n",
    "    print(f\"\\nDigest saved to {filename}\")\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bc96cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Processing Article 1: Perplexity's engineers use 2 AI coding tools, and they've cut development time from days to hours\n",
      "Source: Business Insider\n",
      "URL: https://www.businessinsider.com/perplexity-engineers-ai-tools-cut-development-time-days-hours-2025-7\n",
      "\n",
      "Extracted content length: 62 characters\n",
      "\n",
      "üìù Summary (NEUTRAL):\n",
      "Summary unavailable\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "Processing Article 2: I was laid off from my product management job at Microsoft at 25. It may have been the best thing for me.\n",
      "Source: Business Insider\n",
      "URL: https://www.businessinsider.com/laid-off-microsoft-big-tech-best-thing-2025-7\n",
      "\n",
      "Extracted content length: 62 characters\n",
      "\n",
      "üìù Summary (NEUTRAL):\n",
      "Summary unavailable\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "Processing Article 3: AI Or The Human Touch? Striking A Balance In Customer Retention\n",
      "Source: Forbes\n",
      "URL: https://www.forbes.com/sites/alisoncoleman/2025/07/14/ai-or-the-human-touch-striking-a-balance-in-customer-retention/\n",
      "\n",
      "Extracted content length: 7104 characters\n",
      "Article too long, chunking...\n",
      "Summarizing chunk 1/1...\n",
      "Creating final summary from chunks...\n",
      "\n",
      "üìù Summary (POSITIVE):\n",
      "Here is a 2-paragraph summary of the article:\n",
      "\n",
      "The core innovation in customer retention strategies is the integration of AI and automation, which provides advanced capabilities for predicting customer behavior and personalizing interactions. However, businesses must strike a balance between leveraging AI for efficiency and retaining the impact of empathetic human interaction, as consumers trust humans more despite sharing more personal information with AI agents.\n",
      "\n",
      "Key people and organizations involved in the article include Andrew Griffith of Garden Furniture, Rich Kingly of Driveway King, Berkay Kinaci of Speaktor, and Lindsay Marty of Above the Bar Marketing, who share their approaches to customer retention, including AI-driven segmentation, personalized touches, and predictive churn modeling. Significant data points include a 10% reduction in churn through AI-driven insights, a 20% boost in retention through loyalty rewards, and a 60% return rate for clients who receive personalized follow-ups, highlighting the importance of balancing AI and human interaction in customer retention strategies.\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "Processing Article 4: Promap ‚Äì AI recruitment software\n",
      "Source: Betalist.com\n",
      "URL: https://betalist.com/startups/promap\n",
      "\n",
      "Extracted content length: 254 characters\n",
      "\n",
      "üìù Summary (POSITIVE):\n",
      "Here is a concise 2-paragraph summary of the article:\n",
      "\n",
      "Promap.ai is an innovative AI recruitment software that revolutionizes the hiring process by sourcing, evaluating, and interviewing top talent, resulting in a significant 90% reduction in hiring costs. This AI-powered solution provides bias-free and data-driven results, transforming the traditional recruitment landscape.\n",
      "\n",
      "The key entity involved is Promap.ai, a startup that has developed this cutting-edge technology to transform the recruitment industry. The significance of this innovation lies in its ability to reduce hiring costs and provide unbiased results, making it a game-changer for companies and recruiters.\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "Processing Article 5: The AI labs are coming for Wall Street's quants\n",
      "Source: Business Insider\n",
      "URL: https://www.businessinsider.com/ai-talent-openai-wall-street-quant-trading-firms-2025-7\n",
      "\n",
      "Extracted content length: 62 characters\n",
      "\n",
      "üìù Summary (NEUTRAL):\n",
      "Summary unavailable\n",
      "==================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "DAILY DIGEST REPORT\n",
      "============================================================\n",
      "\n",
      "DAILY NEWS DIGEST: AI STARTUPS\n",
      "Date: 2025-07-15\n",
      "Articles: 5\n",
      "Sentiment: üî• 2 Positive | ‚ö†Ô∏è 3 Neutral | ‚ö° 0 Negative\n",
      "\n",
      "KEY TAKEAWAYS:\n",
      "\n",
      "‚Ä¢ ‚ö†Ô∏è Perplexity's engineers use 2 AI coding tools, and they've cut development time from days to hours (NEUTRAL)\n",
      "   - Summary unavailable\n",
      "   - Source: Business Insider\n",
      "‚Ä¢ ‚ö†Ô∏è I was laid off from my product management job at Microsoft at 25. It may have been the best thing for me. (NEUTRAL)\n",
      "   - Summary unavailable\n",
      "   - Source: Business Insider\n",
      "‚Ä¢ üî• AI Or The Human Touch? Striking A Balance In Customer Retention (POSITIVE)\n",
      "   - Here is a 2-paragraph summary of the article:\n",
      "\n",
      "The core innovation in customer retention strategies is the integration of AI and automation, which provides advanced capabilities for predicting customer behavior and personalizing interactions. However, businesses must strike a balance between leve...\n",
      "   - Source: Forbes\n",
      "‚Ä¢ üî• Promap ‚Äì AI recruitment software (POSITIVE)\n",
      "   - Here is a concise 2-paragraph summary of the article:\n",
      "\n",
      "Promap.ai is an innovative AI recruitment software that revolutionizes the hiring process by sourcing, evaluating, and interviewing top talent, resulting in a significant 90% reduction in hiring costs. This AI-powered solution provides bias-f...\n",
      "   - Source: Betalist.com\n",
      "‚Ä¢ ‚ö†Ô∏è The AI labs are coming for Wall Street's quants (NEUTRAL)\n",
      "   - Summary unavailable\n",
      "   - Source: Business Insider\n",
      "\n",
      "SOURCES:\n",
      "\n",
      "[1] https://www.businessinsider.com/perplexity-engineers-ai-tools-cut-development-time-days-hours-2025-7\n",
      "[2] https://www.businessinsider.com/laid-off-microsoft-big-tech-best-thing-2025-7\n",
      "[3] https://www.forbes.com/sites/alisoncoleman/2025/07/14/ai-or-the-human-touch-striking-a-balance-in-customer-retention/\n",
      "[4] https://betalist.com/startups/promap\n",
      "[5] https://www.businessinsider.com/ai-talent-openai-wall-street-quant-trading-firms-2025-7\n",
      "\n",
      "\n",
      "Digest saved to news_digest_20250715.txt\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Initialize components\n",
    "    load_dotenv()\n",
    "    topic = \"AI Startups\"\n",
    "    \n",
    "    fetcher = NewsFetcher()\n",
    "    summarizer = ArticleSummarizer()\n",
    "    sentiment_analyzer = SentimentAnalyzer(model_name=\"llama3-8b-8192\")  # Smaller model for sentiment\n",
    "    digest_generator = DailyDigestGenerator(topic)\n",
    "    \n",
    "    # Get articles\n",
    "    articles = fetcher.fetch_articles(\n",
    "        query=topic,\n",
    "        num_articles=5,\n",
    "        days_back=1\n",
    "    )\n",
    "    \n",
    "    processed_articles = []\n",
    "    \n",
    "    # Process each article\n",
    "    for i, article in enumerate(articles, 1):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing Article {i}: {article['title']}\")\n",
    "        print(f\"Source: {article['source']}\")\n",
    "        print(f\"URL: {article['url']}\")\n",
    "        \n",
    "        try:\n",
    "            # Summarize content\n",
    "            summary = summarizer.summarize(article)\n",
    "            \n",
    "            # Analyze sentiment\n",
    "            sentiment = sentiment_analyzer.analyze(summary)\n",
    "            \n",
    "            # Store processed article\n",
    "            processed_article = {\n",
    "                \"title\": article['title'],\n",
    "                \"source\": article['source'],\n",
    "                \"url\": article['url'],\n",
    "                \"summary\": summary,\n",
    "                \"sentiment\": sentiment\n",
    "            }\n",
    "            processed_articles.append(processed_article)\n",
    "            \n",
    "            print(f\"\\nüìù Summary ({sentiment}):\")\n",
    "            print(summary)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Processing failed: {str(e)}\")\n",
    "        \n",
    "        print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    # Generate daily digest\n",
    "    if processed_articles:\n",
    "        digest = digest_generator.generate(processed_articles)\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"DAILY DIGEST REPORT\")\n",
    "        print(\"=\"*60)\n",
    "        print(digest)\n",
    "        \n",
    "        # Save to file\n",
    "        filename = f\"news_digest_{datetime.now(timezone.utc).strftime('%Y%m%d')}.txt\"\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(digest)\n",
    "        print(f\"\\nDigest saved to {filename}\")\n",
    "    else:\n",
    "        print(\"No articles processed. Digest not generated.\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46d893b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     62\u001b[39m         f.write(digest)\n\u001b[32m     63\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDigest saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      7\u001b[39m summarizer = ArticleSummarizer(model_name=\u001b[33m\"\u001b[39m\u001b[33mmixtral-8x7b-32768\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m sentiment_analyzer = SentimentAnalyzer(model_name=\u001b[33m\"\u001b[39m\u001b[33mllama3-8b-8192\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# Smaller model for sentiment\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m digest_generator = \u001b[43mDailyDigestGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Get articles\u001b[39;00m\n\u001b[32m     12\u001b[39m articles = fetcher.fetch_articles(\n\u001b[32m     13\u001b[39m     query=topic,\n\u001b[32m     14\u001b[39m     num_articles=\u001b[32m5\u001b[39m,\n\u001b[32m     15\u001b[39m     days_back=\u001b[32m1\u001b[39m\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mDailyDigestGenerator.__init__\u001b[39m\u001b[34m(self, topic)\u001b[39m\n\u001b[32m      7\u001b[39m sentiment_counts = {\u001b[33m\"\u001b[39m\u001b[33mPOSITIVE\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mNEGATIVE\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mNEUTRAL\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m}\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m articles:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     sentiment_counts[\u001b[43marticle\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msentiment\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m] += \u001b[32m1\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Create sentiment summary\u001b[39;00m\n\u001b[32m     12\u001b[39m sentiment_summary = (\n\u001b[32m     13\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müî• \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentiment_counts[\u001b[33m'\u001b[39m\u001b[33mPOSITIVE\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Positive | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚ö†Ô∏è \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentiment_counts[\u001b[33m'\u001b[39m\u001b[33mNEUTRAL\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Neutral | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚ö° \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentiment_counts[\u001b[33m'\u001b[39m\u001b[33mNEGATIVE\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Negative\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[31mKeyError\u001b[39m: 'sentiment'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Initialize components\n",
    "    load_dotenv()\n",
    "    topic = \"AI Startups\"\n",
    "\n",
    "    fetcher = NewsFetcher()\n",
    "    summarizer = ArticleSummarizer(model_name=\"mixtral-8x7b-32768\")\n",
    "    sentiment_analyzer = SentimentAnalyzer(model_name=\"llama3-8b-8192\")  # Smaller model for sentiment\n",
    "    digest_generator = DailyDigestGenerator(topic)\n",
    "\n",
    "    # Get articles\n",
    "    articles = fetcher.fetch_articles(\n",
    "        query=topic,\n",
    "        num_articles=5,\n",
    "        days_back=1\n",
    "    )\n",
    "\n",
    "    processed_articles = []\n",
    "\n",
    "    # Process each article\n",
    "    for i, article in enumerate(articles, 1):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing Article {i}: {article['title']}\")\n",
    "        print(f\"Source: {article['source']}\")\n",
    "        print(f\"URL: {article['url']}\")\n",
    "\n",
    "        try:\n",
    "            # Summarize content\n",
    "            summary = summarizer.summarize(article)\n",
    "            \n",
    "            # Analyze sentiment\n",
    "            sentiment = sentiment_analyzer.analyze(summary)\n",
    "\n",
    "            # Store processed article\n",
    "            processed_article = {\n",
    "                \"title\": article['title'],\n",
    "                \"source\": article['source'],\n",
    "                \"url\": article['url'],\n",
    "                \"summary\": summary,\n",
    "                \"sentiment\": sentiment\n",
    "            }\n",
    "            processed_articles.append(processed_article)\n",
    "            \n",
    "            print(f\"\\nüìù Summary ({sentiment}):\")\n",
    "            print(summary)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Processing failed: {str(e)}\")\n",
    "\n",
    "        print(f\"{'='*50}\\n\")\n",
    "\n",
    "    # Generate daily digest\n",
    "    digest = digest_generator.generate(processed_articles)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DAILY DIGEST REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    print(digest)\n",
    "\n",
    "    # Save to file\n",
    "    filename = f\"news_digest_{datetime.now(timezone.utc).strftime('%Y%m%d')}.txt\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(digest)\n",
    "    print(f\"\\nDigest saved to {filename}\")\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_News_Digest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
